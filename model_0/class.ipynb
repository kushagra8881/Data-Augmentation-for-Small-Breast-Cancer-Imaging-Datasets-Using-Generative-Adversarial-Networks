{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 200 images\n",
      "Class distribution: [ 96 104]\n",
      "Epoch 1/100 | Train Loss: 0.5836 | Train Acc: 0.6562 | Val Loss: 0.5170 | Val Acc: 0.8000 | LR: 1.00e-03\n",
      "Saved new best model\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83        21\n",
      "           1       0.87      0.68      0.76        19\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.81      0.79      0.80        40\n",
      "weighted avg       0.81      0.80      0.80        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 6 13]]\n",
      "Epoch 2/100 | Train Loss: 0.6181 | Train Acc: 0.6312 | Val Loss: 1.6074 | Val Acc: 0.5250 | LR: 1.00e-03\n",
      "Epoch 3/100 | Train Loss: 0.5451 | Train Acc: 0.7063 | Val Loss: 2.3512 | Val Acc: 0.5250 | LR: 1.00e-03\n",
      "Epoch 4/100 | Train Loss: 0.4978 | Train Acc: 0.7562 | Val Loss: 0.8309 | Val Acc: 0.6000 | LR: 1.00e-03\n",
      "Epoch 5/100 | Train Loss: 0.5115 | Train Acc: 0.7250 | Val Loss: 2.2023 | Val Acc: 0.5500 | LR: 5.00e-04\n",
      "Epoch 6/100 | Train Loss: 0.4910 | Train Acc: 0.7750 | Val Loss: 0.4535 | Val Acc: 0.7500 | LR: 5.00e-04\n",
      "Epoch 7/100 | Train Loss: 0.5083 | Train Acc: 0.7188 | Val Loss: 0.3730 | Val Acc: 0.9500 | LR: 5.00e-04\n",
      "Saved new best model\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        21\n",
      "           1       0.95      0.95      0.95        19\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20  1]\n",
      " [ 1 18]]\n",
      "Epoch 8/100 | Train Loss: 0.4706 | Train Acc: 0.7688 | Val Loss: 0.5752 | Val Acc: 0.6750 | LR: 5.00e-04\n",
      "Epoch 9/100 | Train Loss: 0.5450 | Train Acc: 0.7250 | Val Loss: 1.3426 | Val Acc: 0.4750 | LR: 5.00e-04\n",
      "Epoch 10/100 | Train Loss: 0.4794 | Train Acc: 0.7688 | Val Loss: 0.4979 | Val Acc: 0.7500 | LR: 5.00e-04\n",
      "Epoch 11/100 | Train Loss: 0.4541 | Train Acc: 0.7500 | Val Loss: 0.5609 | Val Acc: 0.7250 | LR: 2.50e-04\n",
      "Epoch 12/100 | Train Loss: 0.3735 | Train Acc: 0.8625 | Val Loss: 0.3817 | Val Acc: 0.8250 | LR: 2.50e-04\n",
      "Epoch 13/100 | Train Loss: 0.4369 | Train Acc: 0.7875 | Val Loss: 0.3311 | Val Acc: 0.8750 | LR: 2.50e-04\n",
      "Epoch 14/100 | Train Loss: 0.3974 | Train Acc: 0.8438 | Val Loss: 0.3199 | Val Acc: 0.9000 | LR: 2.50e-04\n",
      "Epoch 15/100 | Train Loss: 0.3568 | Train Acc: 0.8125 | Val Loss: 0.6480 | Val Acc: 0.6750 | LR: 2.50e-04\n",
      "Epoch 16/100 | Train Loss: 0.3814 | Train Acc: 0.8313 | Val Loss: 0.7143 | Val Acc: 0.6250 | LR: 2.50e-04\n",
      "Epoch 17/100 | Train Loss: 0.3799 | Train Acc: 0.8438 | Val Loss: 0.3453 | Val Acc: 0.9250 | LR: 2.50e-04\n",
      "Epoch 18/100 | Train Loss: 0.3634 | Train Acc: 0.8562 | Val Loss: 0.3336 | Val Acc: 0.8750 | LR: 1.25e-04\n",
      "Epoch 19/100 | Train Loss: 0.3738 | Train Acc: 0.8688 | Val Loss: 0.4488 | Val Acc: 0.7000 | LR: 1.25e-04\n",
      "Epoch 20/100 | Train Loss: 0.3324 | Train Acc: 0.8562 | Val Loss: 0.3458 | Val Acc: 0.8000 | LR: 1.25e-04\n",
      "Epoch 21/100 | Train Loss: 0.3457 | Train Acc: 0.8438 | Val Loss: 0.7633 | Val Acc: 0.6500 | LR: 1.25e-04\n",
      "Epoch 22/100 | Train Loss: 0.3174 | Train Acc: 0.8750 | Val Loss: 0.3639 | Val Acc: 0.8500 | LR: 6.25e-05\n",
      "Epoch 23/100 | Train Loss: 0.3491 | Train Acc: 0.8250 | Val Loss: 0.2880 | Val Acc: 0.9250 | LR: 6.25e-05\n",
      "Epoch 24/100 | Train Loss: 0.3176 | Train Acc: 0.8375 | Val Loss: 0.2600 | Val Acc: 0.9250 | LR: 6.25e-05\n",
      "Epoch 25/100 | Train Loss: 0.2824 | Train Acc: 0.9062 | Val Loss: 1.1152 | Val Acc: 0.6000 | LR: 6.25e-05\n",
      "Epoch 26/100 | Train Loss: 0.3122 | Train Acc: 0.8562 | Val Loss: 0.2984 | Val Acc: 0.8250 | LR: 6.25e-05\n",
      "Epoch 27/100 | Train Loss: 0.3532 | Train Acc: 0.8250 | Val Loss: 0.3589 | Val Acc: 0.7750 | LR: 6.25e-05\n",
      "Epoch 28/100 | Train Loss: 0.2873 | Train Acc: 0.8688 | Val Loss: 0.2847 | Val Acc: 0.9000 | LR: 3.13e-05\n",
      "Epoch 29/100 | Train Loss: 0.2661 | Train Acc: 0.9125 | Val Loss: 0.2457 | Val Acc: 0.9000 | LR: 3.13e-05\n",
      "Epoch 30/100 | Train Loss: 0.3343 | Train Acc: 0.8375 | Val Loss: 0.4034 | Val Acc: 0.8000 | LR: 3.13e-05\n",
      "Epoch 31/100 | Train Loss: 0.2674 | Train Acc: 0.9250 | Val Loss: 0.2583 | Val Acc: 0.8750 | LR: 3.13e-05\n",
      "Epoch 32/100 | Train Loss: 0.3119 | Train Acc: 0.8500 | Val Loss: 0.2619 | Val Acc: 0.8750 | LR: 3.13e-05\n",
      "Epoch 33/100 | Train Loss: 0.3305 | Train Acc: 0.8750 | Val Loss: 0.2683 | Val Acc: 0.8750 | LR: 1.56e-05\n",
      "Epoch 34/100 | Train Loss: 0.2837 | Train Acc: 0.8938 | Val Loss: 0.2630 | Val Acc: 0.9000 | LR: 1.56e-05\n",
      "Epoch 35/100 | Train Loss: 0.2539 | Train Acc: 0.9062 | Val Loss: 0.2834 | Val Acc: 0.9000 | LR: 1.56e-05\n",
      "Epoch 36/100 | Train Loss: 0.3072 | Train Acc: 0.8625 | Val Loss: 0.2852 | Val Acc: 0.9000 | LR: 1.56e-05\n",
      "Epoch 37/100 | Train Loss: 0.2695 | Train Acc: 0.9000 | Val Loss: 0.2651 | Val Acc: 0.9000 | LR: 7.81e-06\n",
      "\n",
      "Early stopping at epoch 37\n",
      "\n",
      "Training complete. Best validation accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "VAL_SPLIT = 0.2\n",
    "DATA_PATH = \"output_images/\"\n",
    "LABELS = [\"0\", \"1\"]  # Benign (0) and Malignant (1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Enhanced Dataset Class with proper transform handling\n",
    "class UltrasoundDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.rf_images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        valid_extensions = ('.png', '.jpg', '.jpeg')\n",
    "\n",
    "        for label in LABELS:\n",
    "            rf_path = os.path.join(data_path, label, \"rf\")\n",
    "            if not os.path.exists(rf_path):\n",
    "                continue\n",
    "                \n",
    "            rf_files = natsorted([f for f in os.listdir(rf_path) if f.lower().endswith(valid_extensions)])\n",
    "\n",
    "            for rf_file in rf_files:\n",
    "                rf_img = cv2.imread(os.path.join(rf_path, rf_file), cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if rf_img is None:\n",
    "                    continue\n",
    "                \n",
    "                rf_img = cv2.resize(rf_img, IMG_SIZE)\n",
    "                self.rf_images.append(rf_img)\n",
    "                self.labels.append(int(label))\n",
    "\n",
    "        print(f\"Loaded {len(self.rf_images)} images\")\n",
    "        print(f\"Class distribution: {np.bincount(self.labels)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rf_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.rf_images[idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image only if transforms are specified\n",
    "        if self.transform:\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.tensor(img, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "            \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "# Enhanced Model Architecture\n",
    "class DenseCNN(nn.Module):\n",
    "    def __init__(self, base_channels=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, base_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(base_channels, base_channels*2, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_channels*8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(base_channels*8, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Training Function with Early Stopping\n",
    "def train_model():\n",
    "    # Data augmentation and normalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    # Load dataset\n",
    "    full_dataset = UltrasoundDataset(DATA_PATH)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int((1 - VAL_SPLIT) * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Apply transforms to subsets\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    labels = [full_dataset.labels[i] for i in train_dataset.indices]\n",
    "    class_counts = torch.bincount(torch.tensor(labels))\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    sample_weights = class_weights[labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DenseCNN().to(device)\n",
    "    \n",
    "    # Loss function with class weighting\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 30\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.4f} | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model_des.pth')\n",
    "            print(\"Saved new best model\")\n",
    "            \n",
    "            # Print classification report\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(all_labels, all_preds, target_names=LABELS))\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(confusion_matrix(all_labels, all_preds))\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
